{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arbeitsschutz\n",
      "automotive\n",
      "baubedarf\n",
      "baubeschlage-mobelbeschlage\n",
      "chemisch-technische-produkte\n",
      "dichtungstechnik-kunststoffe\n",
      "elektrik-heisslufttechnik\n",
      "hydraulikschlauche-hydraulikverschraubungen\n",
      "hydrauliksysteme-hydraulikkomponenten\n",
      "item-profilsysteme\n",
      "maschinenelemente\n",
      "pneumatik\n",
      "schlauche-armaturen\n",
      "schmierstoffe-kfz-bedarf\n",
      "schweisstechnik\n",
      "verbindungselemente-schrauben\n",
      "werkstatteinrichtungen-betriebseinrichtungen\n",
      "werkzeuge-maschinen\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from unidecode import unidecode\n",
    "\n",
    "# URL of the website you want to scrape\n",
    "url = 'https://www.haberkorn.com/at/de'  # Replace with the actual URL\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all <span> elements with the specified class\n",
    "    spans = soup.find_all('span', class_='h4 level-text')\n",
    "    \n",
    "    # Extract and process text from each <span> element\n",
    "    processed_texts = [\n",
    "        unidecode(span.get_text(strip=True))\n",
    "        .lower()\n",
    "        .replace(\",\", \"\")\n",
    "        .replace(\" \", \"-\")\n",
    "        .replace(\"ß\", \"ss\")\n",
    "        for span in spans\n",
    "    ]\n",
    "    \n",
    "    # Print each processed text\n",
    "    for text in processed_texts:\n",
    "        print(text)\n",
    "else:\n",
    "    print(f'Failed to retrieve the webpage. Status code: {response.status_code}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arbeitsschutz\n",
      "automotive\n",
      "baubedarf\n",
      "baubeschlaege-moebelbeschlaege\n",
      "chemisch-technische-produkte\n",
      "dichtungstechnik-kunststoffe\n",
      "elektrik-heisslufttechnik\n",
      "hydraulik­schlaeuche-hydraulik­verschraubungen\n",
      "hydraulik­systeme-hydraulik­komponenten\n",
      "item-profilsysteme\n",
      "maschinenelemente\n",
      "pneumatik\n",
      "schlaeuche-armaturen\n",
      "schmierstoffe-kfz-bedarf\n",
      "schweisstechnik\n",
      "verbindungselemente-schrauben\n",
      "werkstatt­einrichtungen-betriebs­einrichtungen\n",
      "werkzeuge-maschinen\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the website you want to scrape\n",
    "url = 'https://www.haberkorn.com/at/de'  # Replace with the actual URL\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all <span> elements with the specified class\n",
    "    spans = soup.find_all('span', class_='h4 level-text')\n",
    "    \n",
    "    # Extract and process text from each <span> element\n",
    "    processed_texts = [\n",
    "        span.get_text(strip=True)\n",
    "        .lower()\n",
    "        .replace(\",\", \"\")\n",
    "        .replace(\" \", \"-\")\n",
    "        .replace(\"ß\", \"ss\")\n",
    "        .replace(\"ö\", \"oe\")\n",
    "        .replace(\"ä\", \"ae\")\n",
    "        .replace(\"ü\", \"ue\")\n",
    "        for span in spans\n",
    "    ]\n",
    "    \n",
    "    # Print each processed text\n",
    "    for text in processed_texts:\n",
    "        print(text)\n",
    "else:\n",
    "    print(f'Failed to retrieve the webpage. Status code: {response.status_code}')\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to categories_and_subcategories.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from unidecode import unidecode\n",
    "import csv\n",
    "\n",
    "# Base URL\n",
    "base_url = 'https://www.haberkorn.com/at/de/'\n",
    "\n",
    "# Initial categories (your output)\n",
    "categories = [\n",
    "    \"arbeitsschutz\", \"automotive\", \"baubedarf\", \"baubeschlaege-moebelbeschlaege\",\n",
    "    \"chemisch-technische-produkte\", \"dichtungstechnik-kunststoffe\", \"elektrik-heisslufttechnik\",\n",
    "    \"hydraulikschlaeuche-hydraulikverschraubungen\", \"hydrauliksysteme-hydraulikkomponenten\",\n",
    "    \"item-profilsysteme\", \"maschinenelemente\", \"pneumatik\", \"schlaeuche-armaturen\",\n",
    "    \"schmierstoffe-kfz-bedarf\", \"schweisstechnik\", \"verbindungselemente-schrauben\",\n",
    "    \"werkstatteinrichtungen-betriebseinrichtungen\", \"werkzeuge-maschinen\"\n",
    "]\n",
    "\n",
    "# Open a CSV file for writing\n",
    "with open('categories_and_subcategories.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Category\", \"Subcategory\"])\n",
    "\n",
    "    for category in categories:\n",
    "        if category == \"automotive\":\n",
    "            # Write category with no subcategories for \"automotive\"\n",
    "            writer.writerow([category, None])\n",
    "            continue\n",
    "        \n",
    "        # Construct the full URL for the category\n",
    "        url = base_url + category\n",
    "        \n",
    "        # Send an HTTP GET request to the category URL\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Parse the HTML content of the page\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Find all <div> elements with class \"h4\"\n",
    "            subcategories = soup.find_all('div', class_='h4')\n",
    "            \n",
    "            # Check if subcategories exist\n",
    "            if subcategories:\n",
    "                for subcategory in subcategories:\n",
    "                    # Extract and clean the subcategory text\n",
    "                    subcategory_text = unidecode(subcategory.get_text(strip=True))\n",
    "                    writer.writerow([category, subcategory_text])\n",
    "            else:\n",
    "                # If no subcategories are found, write null\n",
    "                writer.writerow([category, None])\n",
    "        else:\n",
    "            # If the request fails, write category with no subcategories\n",
    "            writer.writerow([category, None])\n",
    "\n",
    "print(\"Data has been written to categories_and_subcategories.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to categories_and_subcategories.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from unidecode import unidecode\n",
    "import csv\n",
    "import re\n",
    "\n",
    "# Base URL\n",
    "base_url = 'https://www.haberkorn.com/at/de/'\n",
    "\n",
    "# Initial categories (your output)\n",
    "categories = [\n",
    "    \"arbeitsschutz\", \"automotive\", \"baubedarf\", \"baubeschlaege-moebelbeschlaege\",\n",
    "    \"chemisch-technische-produkte\", \"dichtungstechnik-kunststoffe\", \"elektrik-heisslufttechnik\",\n",
    "    \"hydraulikschlaeuche-hydraulikverschraubungen\", \"hydrauliksysteme-hydraulikkomponenten\",\n",
    "    \"item-profilsysteme\", \"maschinenelemente\", \"pneumatik\", \"schlaeuche-armaturen\",\n",
    "    \"schmierstoffe-kfz-bedarf\", \"schweisstechnik\", \"verbindungselemente-schrauben\",\n",
    "    \"werkstatteinrichtungen-betriebseinrichtungen\", \"werkzeuge-maschinen\"\n",
    "]\n",
    "\n",
    "# Function to clean subcategory text\n",
    "def clean_text(text):\n",
    "    text = unidecode(text)  # Remove diacritical marks\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = text.replace(\",\", \"\")  # Remove commas\n",
    "    text = text.replace(\" \", \"-\")  # Replace spaces with hyphens\n",
    "    text = re.sub(r'\\(\\d+\\)', '', text)  # Remove numbers in brackets\n",
    "    return text.strip()  # Remove any leading/trailing whitespace\n",
    "\n",
    "# Open a CSV file for writing\n",
    "with open('categories_and_subcategories(1).csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Category\", \"Subcategory\"])\n",
    "\n",
    "    for category in categories:\n",
    "        if category == \"automotive\":\n",
    "            # Write category with no subcategories for \"automotive\"\n",
    "            writer.writerow([category, None])\n",
    "            continue\n",
    "        \n",
    "        # Construct the full URL for the category\n",
    "        url = base_url + category\n",
    "        \n",
    "        # Send an HTTP GET request to the category URL\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Parse the HTML content of the page\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Find all <div> elements with class \"h4\"\n",
    "            subcategories = soup.find_all('div', class_='h4')\n",
    "            \n",
    "            # Check if subcategories exist\n",
    "            if subcategories:\n",
    "                for subcategory in subcategories:\n",
    "                    # Extract and clean the subcategory text\n",
    "                    raw_text = subcategory.get_text(strip=True)\n",
    "                    cleaned_text = clean_text(raw_text)\n",
    "                    writer.writerow([category, cleaned_text])\n",
    "            else:\n",
    "                # If no subcategories are found, write null\n",
    "                writer.writerow([category, None])\n",
    "        else:\n",
    "            # If the request fails, write category with no subcategories\n",
    "            writer.writerow([category, None])\n",
    "\n",
    "print(\"Data has been written to categories_and_subcategories.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "subcategory_href=\"/at/de/arbeitsschutz/helme\"\n",
    "subcategory_url = base_url + subcategory_href.lstrip('/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.haberkorn.com/at/de/at/de/arbeitsschutz/helme\n"
     ]
    }
   ],
   "source": [
    "print(subcategory_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to categories_and_subcategories.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "\n",
    "# Base URL\n",
    "base_url = 'https://www.haberkorn.com/at/de/'\n",
    "\n",
    "# Updated categories\n",
    "categories = [\n",
    "    \"arbeitsschutz\", \"automotive\", \"baubedarf\", \"baubeschlaege-moebelbeschlaege\",\n",
    "    \"chemisch-technische-produkte\", \"dichtungstechnik-kunststoffe\", \"elektrik-heisslufttechnik\",\n",
    "    \"hydraulikschlaeuche-hydraulikverschraubungen\", \"hydrauliksysteme-hydraulikkomponenten\",\n",
    "    \"item-profilsysteme\", \"maschinenelemente\", \"pneumatik\", \"schlaeuche-armaturen\",\n",
    "    \"schmierstoffe-kfz-bedarf\", \"schweisstechnik\", \"verbindungselemente-schrauben\",\n",
    "    \"werkstatteinrichtungen-betriebseinrichtungen\", \"werkzeuge-maschinen\"\n",
    "]\n",
    "\n",
    "# Function to clean subcategory text\n",
    "def clean_text(text):\n",
    "    text = text.replace(\"\\u00A0\", \" \")  # Replace non-breaking spaces with regular spaces\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = text.replace(\",\", \"\")  # Remove commas\n",
    "    text = text.replace(\" \", \"-\")  # Replace spaces with hyphens\n",
    "    text = re.sub(r'\\(\\d+\\)', '', text)  # Remove numbers in brackets\n",
    "    return text.strip()  # Remove any leading/trailing whitespace\n",
    "\n",
    "# Open a CSV file for writing\n",
    "with open('categories_and_subcategories(2).csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Category\", \"Subcategory\"])\n",
    "\n",
    "    for category in categories:\n",
    "        if category == \"automotive\":\n",
    "            # Write category with no subcategories for \"automotive\"\n",
    "            writer.writerow([category, None])\n",
    "            continue\n",
    "        \n",
    "        # Construct the full URL for the category\n",
    "        url = base_url + category\n",
    "        \n",
    "        # Send an HTTP GET request to the category URL\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Parse the HTML content of the page\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Find all <div> elements with class \"h4\"\n",
    "            subcategories = soup.find_all('div', class_='h4')\n",
    "            \n",
    "            # Check if subcategories exist\n",
    "            if subcategories:\n",
    "                for subcategory in subcategories:\n",
    "                    # Extract and clean the subcategory text\n",
    "                    raw_text = subcategory.get_text(strip=True)\n",
    "                    cleaned_text = clean_text(raw_text)\n",
    "                    writer.writerow([category, cleaned_text])\n",
    "            else:\n",
    "                # If no subcategories are found, write null\n",
    "                writer.writerow([category, None])\n",
    "        else:\n",
    "            # If the request fails, write category with no subcategories\n",
    "            writer.writerow([category, None])\n",
    "\n",
    "print(\"Data has been written to categories_and_subcategories.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
